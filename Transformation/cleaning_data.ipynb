{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import contractions\n",
    "\n",
    "input_csv = \"in/tables/lyrics.csv\"\n",
    "output_csv = \"out/tables/filtered_lyrics.csv\"\n",
    "output_csv_words = \"out/tables/filtered_lyrics_words.csv\"\n",
    "\n",
    "#Set the desired path for NLTK data\n",
    "nltk_path = 'in/tables/'\n",
    "if not os.path.exists(nltk_path):\n",
    "    os.makedirs(nltk_path)\n",
    "\n",
    "# Set the NLTK data path\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "# Load NLTK resources\n",
    "nltk.download('stopwords', download_dir=nltk_path)\n",
    "nltk.download('punkt', download_dir=nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=nltk_path)\n",
    "nltk.download('words', download_dir=nltk_path)\n",
    "\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Set of valid English words\n",
    "english_vocab = set(w.lower() for w in words.words())\n",
    "\n",
    "# Read the input CSV\n",
    "df = pd.read_csv(input_csv, encoding='utf-8')\n",
    "\n",
    "# Load additional stopwords from a CSV file\n",
    "additional_stopwords = set()\n",
    "data = pd.read_csv('in/tables/stopwords.csv', encoding='utf-8', sep=',', decimal='.')\n",
    "# Assuming stopwords are in the first column and there is no header in the CSV\n",
    "additional_stopwords = {word.strip().lower() for word in data.iloc[:, 0] if pd.notna(word)}\n",
    "\n",
    "# Prepare stopwords by combining NLTK  and additional stopwords\n",
    "stop_words = set(stopwords.words('english')) | additional_stopwords\n",
    "\n",
    "allowed_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', \n",
    "'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'MD', 'RB', 'RBR', 'RBS'}\n",
    "\n",
    "def filter_pos_tags(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    return [word for word, tag in tagged_tokens if tag in allowed_tags]\n",
    "\n",
    "# Expand contractions    \n",
    "df['lyrics'] = df['lyrics'].apply(lambda x: contractions.fix(str(x)))\n",
    "\n",
    "# Remove unwanted characters but keep the hyphen\n",
    "df[\"lyrics\"] = df[\"lyrics\"].str.replace(r\"[^a-zA-Z\\s\\-]\", \"\", regex=True).str.lower()\n",
    "# Remove remaining unwanted characters\n",
    "df[\"lyrics\"] = df[\"lyrics\"].str.replace(\"-\", \" \")\n",
    "\n",
    "# Count total words before filtering stopwords and pos-tagging\n",
    "df['total_words'] = df['lyrics'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "# Count unique words before filtering stopwords and pos-tagging\n",
    "df['unique_words'] = df['lyrics'].apply(lambda x: len(set(word_tokenize(x))))\n",
    "\n",
    "# Remove stopwords\n",
    "df['filtered_lyrics'] = df['lyrics'].apply(lambda x: [word for word in word_tokenize(x) if word in english_vocab and word not in stop_words])\n",
    "\n",
    "# Filter by part-of-speech tags\n",
    "df['filtered_lyrics'] = df['filtered_lyrics'].apply(lambda x: filter_pos_tags(' '.join(x)))\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "\n",
    "# Create list of tuples (title, artist, word)\n",
    "words_list = [\n",
    "    (title, artist, word)\n",
    "    for title, artist, words in zip(df[\"song\"], df[\"artist\"], df[\"filtered_lyrics\"])\n",
    "    for word in words\n",
    "]\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "df_words = pd.DataFrame(words_list, columns=[\"song\", \"artist\", \"word\"])\n",
    "\n",
    "output_csv = 'filtered_lyrics_words.csv'\n",
    "df_words.to_csv(output_csv_words, index=False, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
